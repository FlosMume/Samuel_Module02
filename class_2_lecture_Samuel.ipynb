{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 Practice: From Transformers to Alignment\n",
    "### Learning Objectives:\n",
    "* Understand attention mechanisms through NumPy code\n",
    "* Build a simple transformer block\n",
    "* Predict next token using a pretrained LLM\n",
    "* Analyze hallucinations\n",
    "* Explore supervised fine-tuning logic\n",
    "* Understand how DPO works via preference modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Package Introduction\n",
    "\n",
    "In this notebook, we will use several important Python libraries:\n",
    "\n",
    "- **[NumPy](https://education.launchcode.org/data-analysis-curriculum/eda-with-pandas/reading/numpy-intro/index.html?utm_term=launchcode&utm_campaign=&utm_source=bing&utm_medium=ppc&hsa_acc=4368208516&hsa_cam=568518766&hsa_grp=1173180668353233&hsa_ad=&hsa_src=o&hsa_tgt=dat-2325123495982042:loc-190&hsa_kw=launchcode&hsa_mt=b&hsa_net=adwords&hsa_ver=3&msclkid=f69fad9bed3f18d44e31c5a6703d580b&utm_content=Group%202)**: The fundamental package for scientific computing with Python. We use it for matrix operations and to demonstrate the attention mechanism.\n",
    "- **[PyTorch](https://www.geeksforgeeks.org/start-learning-pytorch-for-beginners/)**: A popular deep learning framework. We use it to build and train neural network models, including transformer blocks.\n",
    "- **[Hugging Face Transformers](https://github.com/huggingface/transformers)**: Provides state-of-the-art pre-trained models and tools for natural language processing. We use it to load and interact with large language models (LLMs).\n",
    "- **[huggingface-cli](https://huggingface.co/docs/huggingface_hub/main/en/guides/cli)**: A command-line tool for managing Hugging Face models and datasets. Useful for downloading models or checking your authentication.\n",
    "\n",
    "Make sure you have these packages installed. You can install them using pip if needed:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "# Optional: If you're still having issues, you can also try setting max_split_size_mb\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\ch939\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: torch in c:\\users\\ch939\\anaconda3\\lib\\site-packages (2.7.1+cu128)\n",
      "Requirement already satisfied: transformers in c:\\users\\ch939\\anaconda3\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\ch939\\anaconda3\\lib\\site-packages (0.34.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "usage: huggingface-cli <command> [<args>]\n",
      "\n",
      "positional arguments:\n",
      "  {download,upload,repo-files,env,login,whoami,logout,auth,repo,lfs-enable-largefiles,lfs-multipart-upload,scan-cache,delete-cache,tag,version,upload-large-folder}\n",
      "                        huggingface-cli command helpers\n",
      "    download            Download files from the Hub\n",
      "    upload              Upload a file or a folder to a repo on the Hub\n",
      "    repo-files          Manage files in a repo on the Hub\n",
      "    env                 Print information about the environment.\n",
      "    login               Log in using a token from\n",
      "                        huggingface.co/settings/tokens\n",
      "    whoami              Find out which huggingface.co account you are logged\n",
      "                        in as.\n",
      "    logout              Log out\n",
      "    auth                Other authentication related commands\n",
      "    repo                {create} Commands to interact with your huggingface.co\n",
      "                        repos.\n",
      "    lfs-enable-largefiles\n",
      "                        Configure your repository to enable upload of files >\n",
      "                        5GB.\n",
      "    scan-cache          Scan cache directory.\n",
      "    delete-cache        Delete revisions from the cache directory.\n",
      "    tag                 (create, list, delete) tags for a repo in the hub\n",
      "    version             Print information about the huggingface-cli version.\n",
      "    upload-large-folder\n",
      "                        Upload a large folder to a repo on the Hub\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n"
     ]
    }
   ],
   "source": [
    "! pip install numpy torch transformers huggingface_hub\n",
    "\n",
    "# For `huggingface-cli`, it is included with `huggingface_hub` or `transformers`. You can check your installation with:\n",
    "\n",
    "! huggingface-cli --help\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Attention Mechanism (Self-Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Random Q, K, V matrices\n",
    "def generate_random_qkv(seq_len=4, d_model=8):\n",
    "    return [np.random.rand(seq_len, d_model) for _ in range(3)]\n",
    "\n",
    "# Scaled dot-product attention\n",
    "def self_attention(Q, K, V):\n",
    "    d_k = Q.shape[-1]\n",
    "    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n",
    "    weights = softmax(scores)\n",
    "    output = np.dot(weights, V)\n",
    "    return output, weights\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "Q, K, V = generate_random_qkv()\n",
    "out, attn_weights = self_attention(Q, K, V)\n",
    "print(\"Attention Output:\\n\", out)\n",
    "print(\"Attention Weights:\\n\", attn_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. import numpy as np:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line imports the numpy library, which is essential for numerical operations, especially array and matrix manipulations, that are heavily used in this code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. generate_random_qkv(seq_len=4, d_model=8) function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Purpose:__ This function creates three random matrices: Query (Q), Key (K), and Value (V). These matrices are conceptual representations of the input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Parameters:__\n",
    "* seq_len: Represents the \"sequence length,\" which is the number of tokens or elements in the input sequence. Here, it's defaulted to 4.\n",
    "* d_model: Represents the \"dimension of the model\" or the embedding dimension of each token. Here, it's defaulted to 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Return:__ It returns a list containing three NumPy arrays, each of shape (seq_len, d_model). These arrays are filled with random floating-point numbers. In a real-world scenario, these would be learned embeddings of your input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. softmax(x) function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Purpose:__ This function implements the softmax activation function. Softmax is crucial for converting a vector of arbitrary real values into a probability distribution, where all elements are non-negative and sum to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Parameters:__\n",
    "* x: A NumPy array (typically a vector or a matrix)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Mechanism:__\n",
    "\n",
    "* exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True)): To ensure numerical stability and prevent potential overflow issues when computing exp(x) for large values, the maximum value in x along the last axis is subtracted from x before applying the exponential function. keepdims=True ensures the subtracted maximum maintains its dimension, allowing for proper broadcasting.\n",
    "\n",
    "* return exp_x / np.sum(exp_x, axis=-1, keepdims=True): Each element of exp_x is then divided by the sum of all elements along the last axis. This normalizes the values into a probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. self_attention(Q, K, V) function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Purpose:__ This is the core implementation of the Scaled Dot-Product Attention mechanism. It calculates how much attention each element in the sequence should pay to other elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Parameters:__\n",
    "\n",
    "* Q: The Query matrix.\n",
    "\n",
    "* K: The Key matrix.\n",
    "\n",
    "* V: The Value matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Mechanisms:__\n",
    "* d_k = Q.shape[-1]: d_k is the dimension of the key vectors (which is the same as the query vectors). This is used for scaling.\n",
    "* scores = np.dot(Q, K.T) / np.sqrt(d_k): This is the \"dot-product\" part.\n",
    "\n",
    "    1. np.dot(Q, K.T): Computes the dot product between the Query matrix and the transpose of the Key matrix. This measures the similarity or \"relevance\" between each query vector and all key vectors. The result is a (seq_len, seq_len) matrix, where each element (i, j) represents the similarity between the i-th query and the j-th key.\n",
    "\n",
    "    2. / np.sqrt(d_k): The scores are divided by the square root of d_k. This \"scaling\" factor is crucial to prevent the dot products from becoming too large, especially with high-dimensional d_k, which can push the softmax function into regions with extremely small gradients, leading to vanishing gradients.\n",
    "\n",
    "* weights = softmax(scores): The scores are passed through the softmax function. This converts the raw similarity scores into attention weights. Each row of weights represents how much attention a particular query element pays to all other key elements in the sequence. The sum of weights in each row will be 1.\n",
    "\n",
    "* output = np.dot(weights, V): This is the \"weighted sum\" part. The attention weights are multiplied by the Value matrix. This operation essentially creates a weighted sum of the Value vectors, where the weights determine the importance of each Value vector for generating the output for a specific query. The output captures the relevant information from the input sequence, weighted by the attention scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Return:__ It returns two things:\n",
    "\n",
    "* output: The final attention output matrix, with the same shape as V (seq_len, d_model).\n",
    "\n",
    "* weights: The attention weights matrix (seq_len, seq_len)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example Usage:__\n",
    "\n",
    "* Q, K, V = generate_random_qkv(): Calls the function to generate random Query, Key, and Value matrices.\n",
    "\n",
    "* out, attn_weights = self_attention(Q, K, V): Calls the self_attention function with the generated Q, K, and V, storing the results in out (the attention output) and attn_weights (the attention weights).\n",
    "\n",
    "* print(\"Attention Output:\\n\", out): Prints the resulting attention output matrix.\n",
    "\n",
    "* print(\"Attention Weights:\\n\", attn_weights): Prints the attention weights matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In essence, this code demonstrates how self-attention works:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For each element in an input sequence (represented by its Query vector), it calculates its similarity to all other elements (represented by their Key vectors). \n",
    "* These similarities are then scaled and transformed into probabilities (attention weights) using softmax. \n",
    "* Finally, these attention weights are used to compute a weighted sum of the Value vectors, producing an output that \"attends\" to the most relevant parts of the input sequence. \n",
    "* This mechanism allows the model to capture long-range dependencies and contextual relationships within the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion:\n",
    "Walk students through the QK^T score computation, scaling, and softmax. Explain how this captures relationships between tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Mini Transformer Block in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MiniTransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads=2, batch_first=True)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim * 4, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(x + attn_output)\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_output)\n",
    "        return x\n",
    "\n",
    "x = torch.randn(1, 5, 16)  # batch_size=1, seq_len=5, embed_dim=16\n",
    "model = MiniTransformerBlock(embed_dim=16)\n",
    "out = model(x)\n",
    "print(out.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Goal: Show how self-attention and FFN work with residual and norm in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* torch: This is the main PyTorch library, providing fundamental tensor operations and utilities.\n",
    "\n",
    "* torch.nn: This sub-library provides modules for building neural networks, including layers like linear transformations, activation functions, and attention mechanisms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. MiniTransformerBlock Class\n",
    "This class inherits from nn.Module, which is the base class for all neural network modules in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__init__(self, embed_dim)\n",
    "This is the constructor method where the layers of the Transformer block are defined.\n",
    "\n",
    "* super().__init__(): Calls the constructor of the parent class nn.Module. This is crucial for proper initialization of PyTorch modules.\n",
    "\n",
    "* self.attn = nn.MultiheadAttention(embed_dim, num_heads=2, batch_first=True):\n",
    "\n",
    "    * This defines the Multi-Head Attention mechanism.\n",
    "\n",
    "    * embed_dim: This is the dimension of the input and output features for each token in the sequence.\n",
    "\n",
    "    * num_heads=2: This specifies that the attention mechanism will be split into 2 \"heads.\" Each head can learn different aspects of the relationships between tokens. Their outputs are then concatenated and linearly transformed.\n",
    "\n",
    "    * batch_first=True: This tells the MultiheadAttention module that the input tensor will have the batch dimension as the first dimension (e.g., [batch_size, sequence_length, embedding_dimension]).\n",
    "\n",
    "* self.ffn = nn.Sequential(...):\n",
    "\n",
    "    * This defines the Feed-Forward Network (FFN), which is applied independently to each position in the sequence. It's wrapped in nn.Sequential to define a sequence of operations.\n",
    "\n",
    "    * nn.Linear(embed_dim, embed_dim * 4): A linear (dense) layer that projects the embed_dim input to a higher dimension (embed_dim * 4).\n",
    "\n",
    "    * nn.ReLU(): The Rectified Linear Unit (ReLU) activation function, which introduces non-linearity.\n",
    "\n",
    "    * nn.Linear(embed_dim * 4, embed_dim): Another linear layer that projects the higher-dimensional output back to the original embed_dim.\n",
    "\n",
    "* self.norm1 = nn.LayerNorm(embed_dim):\n",
    "\n",
    "    * This defines the first Layer Normalization layer. Layer normalization normalizes the inputs across the feature dimension for each sample independently. This helps stabilize training.\n",
    "\n",
    "* self.norm2 = nn.LayerNorm(embed_dim):\n",
    "\n",
    "    * This defines the second Layer Normalization layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Next Token Prediction using HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1: Use a Publicly Available Model\n",
    "Use a non-gated model such as TinyLlama or mistralai/Mistral-7B-v0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "datasets 3.0.1 requires pyarrow>=15.0.0, but not pyarrow 14.0.2. To avoid dependency Conflict (PyArrow): datasets 3.0.1 requires pyarrow>=15.0.0, but you have pyarrow 14.0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pyarrow>=15.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tell Python to use UTF-8 for I/O operations within your Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTHONIOENCODING\"] = \"utf-8\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Python's login() Function to save the Huggingface token to Git credential helper. Git version 2.50.1.windows.1 has been installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Get your token from Hugging Face website (Settings -> Access Tokens)\n",
    "your_huggingface_token = \"MyToken\" # The placeholder for my token\n",
    "\n",
    "login(token=your_huggingface_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub[cli] in c:\\users\\ch939\\anaconda3\\lib\\site-packages (0.34.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from huggingface_hub[cli]) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from huggingface_hub[cli]) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from huggingface_hub[cli]) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from huggingface_hub[cli]) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from huggingface_hub[cli]) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from huggingface_hub[cli]) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from huggingface_hub[cli]) (4.14.1)\n",
      "Requirement already satisfied: InquirerPy==0.3.4 in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from huggingface_hub[cli]) (0.3.4)\n",
      "Requirement already satisfied: pfzy<0.4.0,>=0.3.1 in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (0.3.4)\n",
      "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (3.0.43)\n",
      "Requirement already satisfied: colorama in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub[cli]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from requests->huggingface_hub[cli]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from requests->huggingface_hub[cli]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from requests->huggingface_hub[cli]) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from requests->huggingface_hub[cli]) (2025.4.26)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface_hub[cli]) (0.2.5)\n",
      "\u001b[33mâš ï¸�  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `hf`CLI if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "The token `Read30July` has been saved to C:\\Users\\ch939\\.cache\\huggingface\\stored_tokens\n",
      "Your token has been saved to C:\\Users\\ch939\\.cache\\huggingface\\token\n",
      "Login successful.\n",
      "The current active token is: `Read30July`\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Login via CLI\n",
    "! pip install --upgrade huggingface_hub[cli]\n",
    "# To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
    "! huggingface-cli login --token MyToken\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you want to optimize and scale your deep learning workflows, even for publicly available models, __accelerate__ becomes highly beneficial and often necessary. Using a `device_map` or `tp_plan` requires `accelerate`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A __device_map__ is essentially a plan or a blueprint that tells accelerate (and by extension, transformers) where to place each individual layer (or component) of your large language model across your available computational devices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__tp_plan__ stands for Tensor Parallelism Plan. This is a more advanced technique used for distributed inference and training of extremely large models, especially when a single layer of the model itself is too large to fit on a single GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below imstallation can be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install hf_xet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import ipywidgets as widgets;\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that I have an NVIDIA GeForce RTX 4070 SUPER (which has either 12GB or 16GB of VRAM) and accelerate is installed and I've accepted the model terms, the most probable cause for the continuous \"Error displaying widget\" during model loading is now:\n",
    "\n",
    "__Insufficient GPU Memory (VRAM) for the model's default loading precision.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mistral-7B, even with device_map=\"auto\", might be attempting to load in float32 (full precision) by default, which can exceed the VRAM of a 4070 SUPER."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution: Load the Model in Lower Precision (8-bit or 4-bit)__\n",
    "\n",
    "You need to tell transformers to load the model in a more memory-efficient format. This is typically done using bitsandbytes for 8-bit or 4-bit quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1+cu128\n",
      "12.8\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "# login(token=\"MyToken\")  # optional if already logged in via CLI\n",
    "\n",
    "import torch # Make sure to import torch for torch_dtype\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None for CUDA was the root cause of all my previous GPU-related errors. Even though you have a powerful RTX 4070 SUPER, your Python environment's PyTorch installation is incapable of communicating with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve the problem, I uninstalled my previous CPU-only PyTorch in Anaconda.\n",
    "\n",
    "pip uninstall torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Determine the Correct PyTorch Installation Command:__\n",
    "\n",
    "Go to the official PyTorch website to get the most accurate and up-to-date installation command for my system:\n",
    "https://pytorch.org/get-started/locally/\n",
    "\n",
    "On this page, select the following options:\n",
    "\n",
    "PyTorch Build: Stable\n",
    "\n",
    "Your OS: Windows\n",
    "\n",
    "Package: Pip\n",
    "\n",
    "Compute Platform: CUDA 12.1 (or the highest available CUDA version that is 12.x for my RTX 4070 SUPER). CUDA 12.1 or 12.2 is generally compatible with the 40-series cards. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA version 12.8 is selected. The website will then provide you with the exact pip install command to install the CUDA-enabled version."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# I have to visit https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1 to sign the agreement in order to use this model\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, token=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this was the previous command used:\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, token=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1+cu128\n",
      "12.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install bitsandbytes using the Gohlke wheel (if PyTorch is CUDA-enabled):\n",
    "This remains the most robust method for Windows.\n",
    "\n",
    "* Go to https://www.lfd.uci.edu/~gohlke/pythonlibs/#bitsandbytes\n",
    "\n",
    "* Download the .whl file that matches your Python version and the CUDA version your PyTorch is built with (e.g., if PyTorch is 2.3.0+cu121 and your Python is 3.10, look for cp310 and cu121).\n",
    "\n",
    "* Navigate to your Downloads folder in your Anaconda Prompt/terminal (not necessarily Jupyter for this step, though !pip install in Jupyter can work if you provide the full path to the .whl file).\n",
    "\n",
    "* Run:\n",
    "\n",
    "Bash\n",
    "\n",
    "pip install C:\\Users\\YourUser\\Downloads\\your_downloaded_bitsandbytes_wheel_file.whl\n",
    "\n",
    "Replace the path and filename with your actual downloaded file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the website is out of service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prioritized Approach (No Gohlke wheels needed if this works):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1+cu128\n",
      "True\n",
      "12.8\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Using cached bitsandbytes-0.46.1-py3-none-win_amd64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: accelerate in c:\\users\\ch939\\anaconda3\\lib\\site-packages (1.9.0)\n",
      "Requirement already satisfied: torch<3,>=2.2 in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from bitsandbytes) (2.7.1+cu128)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from accelerate) (0.34.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (2024.3.1)\n",
      "Requirement already satisfied: requests in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (69.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.4.26)\n",
      "Using cached bitsandbytes-0.46.1-py3-none-win_amd64.whl (72.2 MB)\n",
      "Installing collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.46.1\n"
     ]
    }
   ],
   "source": [
    "!pip install bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from transformers import BitsAndBytesConfig as bnb_config\n",
    "import torch\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Run my Model Loading Code with BitsAndBytesConfig:__\n",
    "\n",
    "After the kernel restarts, run the following code. This uses the BitsAndBytesConfig which is the recommended way to load models in 8-bit or 4-bit, and bitsandbytes should now correctly detect your CUDA-enabled GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer for mistralai/Mistral-7B-Instruct-v0.1...\n",
      "Tokenizer loaded.\n",
      "Loading model mistralai/Mistral-7B-Instruct-v0.1 with quantization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ch939\\anaconda3\\Lib\\site-packages\\accelerate\\utils\\modeling.py:821: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  _ = torch.tensor([0], device=i)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "876feffa72b24089884db6a4890dbcea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from huggingface_hub import login\n",
    "import torch # Make sure to import torch\n",
    "\n",
    "# Define your quantization configuration\n",
    "# For 8-bit loading:\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True\n",
    ")\n",
    "\n",
    "# OR, for 4-bit loading (often better for RTX 40-series cards, try this if 8-bit still gives OOM):\n",
    "# This configuration is generally recommended for RTX 40-series for optimal 4-bit performance\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\", # Use NormalFloat4 quantization\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16, # Use bfloat16 for faster computation on 40-series GPUs\n",
    "#     bnb_4bit_use_double_quant=True, # Apply double quantization\n",
    "# )\n",
    "\n",
    "# You've already accepted the agreement on the HF website for Mistral-7B-Instruct-v0.1.\n",
    "# login(token=\"MyToken\") # Optional if already logged in via CLI\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "print(f\"Loading tokenizer for {model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=True)\n",
    "print(\"Tokenizer loaded.\")\n",
    "\n",
    "print(f\"Loading model {model_name} with quantization...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    token=True,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config # Pass the defined bnb_config here\n",
    ")\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# You can then test it\n",
    "# prompt = \"What is the capital of France?\"\n",
    "# inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "# outputs = model.generate(inputs, max_new_tokens=50)\n",
    "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables set and cache cleared.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Environment variables set and cache cleared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "\n",
    "# Define your 4-bit quantization configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "# login(token=\"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\") # Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer for mistralai/Mistral-7B-Instruct-v0.1...\n",
      "Tokenizer loaded.\n",
      "Loading model mistralai/Mistral-7B-Instruct-v0.1 with 4-bit quantization and device_map='auto'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2f64e6624cc450f80e26325e5c7513a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "CUDA memory allocated after model load: 7.20 GB\n",
      "CUDA memory reserved after model load: 7.97 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading tokenizer for {model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=True)\n",
    "print(\"Tokenizer loaded.\")\n",
    "\n",
    "print(f\"Loading model {model_name} with 4-bit quantization and device_map='auto'...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    token=True,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Verify memory usage after loading\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA memory allocated after model load: {torch.cuda.memory_allocated() / (1024**3):.2f} GB\")\n",
    "    print(f\"CUDA memory reserved after model load: {torch.cuda.memory_reserved() / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should now succeed without OOM. The Error displaying widget might still appear cosmetically, but the \"Model loaded successfully!\" is the true indicator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating response...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m    Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:284\u001b[0m, in \u001b[0;36mBatchEncoding.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[item]\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'shape'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating response...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 18\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m     19\u001b[0m         inputs,\n\u001b[0;32m     20\u001b[0m         max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, \u001b[38;5;66;03m# Start with a lower number, increase if successful\u001b[39;00m\n\u001b[0;32m     21\u001b[0m         do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     22\u001b[0m         temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m,\n\u001b[0;32m     23\u001b[0m         top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m\n\u001b[0;32m     24\u001b[0m     )\n\u001b[0;32m     26\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Generated Output ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:2243\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[0;32m   2239\u001b[0m \u001b[38;5;66;03m# 3. Define model inputs\u001b[39;00m\n\u001b[0;32m   2240\u001b[0m inputs_tensor, model_input_name, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_model_inputs(\n\u001b[0;32m   2241\u001b[0m     inputs, generation_config\u001b[38;5;241m.\u001b[39mbos_token_id, model_kwargs\n\u001b[0;32m   2242\u001b[0m )\n\u001b[1;32m-> 2243\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m inputs_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   2245\u001b[0m device \u001b[38;5;241m=\u001b[39m inputs_tensor\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m   2246\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_special_tokens(generation_config, kwargs_has_attention_mask, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:286\u001b[0m, in \u001b[0;36mBatchEncoding.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[item]\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m--> 286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gc  # <--- Make sure this is here for inference\n",
    "import torch # Re-import torch if this is a separate cell and not already imported\n",
    "\n",
    "prompt = \"What is the capital of France?\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device) # Use model.device for correct placement\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Generating response...\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_new_tokens=100, # Start with a lower number, increase if successful\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"\\n--- Generated Output ---\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating response...\n",
      "\n",
      "--- Generated Output ---\n",
      "[INST] Write a short, engaging story about a lost cat who finds its way home. [/INST] Once upon a time, in a cozy little town nestled between rolling hills, there lived a cat named Whiskers. Whiskers was not your ordinary cat; she had a special talent for getting lost. No matter how carefully her owner, Mrs. Smith, tried to keep track of her, Whiskers always seemed to find her way into the most unexpected places.\n",
      "\n",
      "One sunny morning, Whiskers decided to go for a walk. She loved exploring the town and meeting new people (and cats). But this time, she took a turn down a strange alleyway and found herself face-to-face with a mysterious-looking door. Without thinking twice, Whiskers stepped through the door and was suddenly transported to an entirely different world.\n",
      "\n",
      "The new world was bright and colorful, filled with strange creatures and wondrous sights. Whiskers was terrified at first, but her adventurous spirit soon took over. She began to\n"
     ]
    }
   ],
   "source": [
    "# Ensure these are imported if you are running this in a new cell\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# Define your prompt using the message format expected by instruct models\n",
    "prompt = \"Write a short, engaging story about a lost cat who finds its way home.\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "# Apply the chat template to format the prompt correctly for Mistral-Instruct\n",
    "input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs_tokenized = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Explicitly move input_ids and attention_mask to the correct device\n",
    "# model.device will be the primary device assigned by device_map=\"auto\" (e.g., 'cuda:0')\n",
    "input_ids = inputs_tokenized[\"input_ids\"].to(model.device)\n",
    "attention_mask = inputs_tokenized[\"attention_mask\"].to(model.device)\n",
    "\n",
    "\n",
    "# Clear cache before generation (good practice for memory management)\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Generating response...\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,          # Pass input_ids explicitly\n",
    "        attention_mask=attention_mask, # Pass attention_mask explicitly\n",
    "        max_new_tokens=200,           # Adjust as needed (start low if OOM still occurs here)\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        # Add pad_token_id to avoid warnings for some models when batching\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\n--- Generated Output ---\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Comment__\n",
    "Some are previous comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error displaying widget: This message appeared during the model loading phase, which means the progress bar widget (usually provided by tqdm and displayed using ipywidgets in Jupyter) failed to render correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model loaded successfully!: This is the most important part! It confirms that despite the progress bar error, the model was successfully downloaded and loaded into your GPU's memory using the quantization you configured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Error displaying widget is now a cosmetic issue with the progress bar, not a critical error preventing the model from loading. It means the interactive progress bar couldn't show up, but the underlying process completed. This often happens if the Jupyter frontend (your browser's view) doesn't perfectly load the ipywidgets JavaScript components for tqdm's notebook integration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Expected Outcome:__\n",
    "\n",
    "With PyTorch correctly using CUDA and bitsandbytes enabled for GPU quantization, the model.from_pretrained() call should now proceed with downloading the model (if not cached) and loading it into your GPU memory using 8-bit (or 4-bit) precision. The \"Error displaying widget\" related to the progress bar should no longer appear, and you should see download progress and then the \"Model loaded successfully!\" message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're using a Mac with M1/M2/M3 and have this line working:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "! python -c \"import torch; print(torch.backends.mps.is_available())\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it returns True, then you can run like this:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "\n",
    "# Optional if already logged in via CLI\n",
    "# login(token=\"your_hf_token\")\n",
    "\n",
    "# Check device for MacBook (MPS if available, else CPU)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "# Load tokenizer and model with Hugging Face gated repo access\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, token=True).to(device)\n",
    "\n",
    "# Prepare input prompt\n",
    "prompt = \"The Eiffel Tower is located in\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Run generation\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are not sure about your GPU in your device, selects the best available device in the order: CUDA → MPS → CPU and also prints which one it chose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using CUDA (GPU)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ffbb85d24ec499ea2e12139fdd42b0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Generated Output: The Eiffel Tower is located in Paris, France, and is one of the most\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "\n",
    "# Optional if already logged in via CLI\n",
    "# login(token=\"your_hf_token\")\n",
    "\n",
    "# ------------------------------------------\n",
    "# 📦 Device Selection: CUDA > MPS > CPU\n",
    "# ------------------------------------------\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"✅ Using CUDA (GPU)\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"🟡 Using MPS (Apple Silicon GPU)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"🔴 Using CPU\")\n",
    "\n",
    "# ------------------------------------------\n",
    "# 🧠 Load Model from Hugging Face\n",
    "# ------------------------------------------\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    token=True,\n",
    "    torch_dtype=torch.float16 if device.type != \"cpu\" else torch.float32  # avoid FP16 on CPU\n",
    ").to(device)\n",
    "\n",
    "# ------------------------------------------\n",
    "# 📝 Prompt + Inference\n",
    "# ------------------------------------------\n",
    "prompt = \"The Eiffel Tower is located in\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "    print(\"📝 Generated Output:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: DPO vs PPO – Side-by-Side Educational Example\n",
    "#### DPO: Direct Preference Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPO Loss: 4.5398901420412585e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Simulated log-probs of chosen vs rejected completions\n",
    "chosen_logp = torch.tensor([[-1.0]])\n",
    "rejected_logp = torch.tensor([[-2.0]])\n",
    "\n",
    "def dpo_loss(chosen_logp, rejected_logp, beta=0.1):\n",
    "    return -F.logsigmoid((chosen_logp - rejected_logp) / beta).mean()\n",
    "\n",
    "print(\"DPO Loss:\", dpo_loss(chosen_logp, rejected_logp).item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PPO: Proximal Policy Optimization (simplified for in-class demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO Loss: -1.2000000476837158\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Simulated old and new policy log-probs (log π_θ(a|s) and log π_θ_old(a|s))\n",
    "old_log_prob = torch.tensor([[-1.0]])  # from reference policy (e.g. GPT-4 before PPO step)\n",
    "new_log_prob = torch.tensor([[-0.8]])  # from updated policy\n",
    "reward = torch.tensor([[1.0]])         # reward from human or reward model\n",
    "epsilon = 0.2                          # PPO clipping parameter\n",
    "\n",
    "# Compute ratio of new to old policy\n",
    "log_ratio = new_log_prob - old_log_prob\n",
    "ratio = torch.exp(log_ratio)\n",
    "\n",
    "# Unclipped and clipped advantages\n",
    "advantage = reward  # assume reward ~ advantage for simplicity\n",
    "clipped_ratio = torch.clamp(ratio, 1 - epsilon, 1 + epsilon)\n",
    "\n",
    "# PPO loss (negative of the clipped surrogate objective)\n",
    "ppo_loss = -torch.min(ratio * advantage, clipped_ratio * advantage).mean()\n",
    "print(\"PPO Loss:\", ppo_loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 DPO vs PPO: Alignment Loss Comparison\n",
    "\n",
    "| Criterion         | DPO (Direct Preference Optimization)     | PPO (Proximal Policy Optimization)        |\n",
    "|------------------|-------------------------------------------|--------------------------------------------|\n",
    "| 🧠 Origin         | Preference modeling (UnfoldAI 2023)        | Reinforcement Learning (OpenAI 2017)       |\n",
    "| ✅ Rejection Signal | Yes — uses chosen vs rejected pairs       | No — requires scalar reward                |\n",
    "| 🏆 Reward Signal   | Implicit via logit difference              | Explicit reward model needed               |\n",
    "| 📐 Loss Function   | `-log(sigmoid((chosen - rejected)/β))`     | `-min(ratio * A, clipped_ratio * A)`       |\n",
    "| 🔧 Optimization    | Binary classification over preferences     | Policy gradient with clipped surrogate     |\n",
    "| 🎯 Application     | DPO-tuned models like LLaMA 3              | RLHF-tuned models like InstructGPT         |\n",
    "| ⚙️ Complexity      | Simpler (no reward model needed)           | More complex (needs reward model + sampling) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain how aligning models toward human preference uses logit differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Inference with Quantization (O1 & O3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model with torch_dtype=torch.float16 for O1\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concept: Explain how FP16/O1 optimizes memory and speed at inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧠 Concept Breakdown: How FP16 & O1 Optimize Inference\n",
    "\n",
    "#### 📌 What is FP16?\n",
    "\n",
    "- **FP16** = *16-bit floating point*, also called **half precision**.\n",
    "- It uses **less memory** than FP32 (standard 32-bit float), with:\n",
    "  - 1 sign bit\n",
    "  - 5 exponent bits\n",
    "  - 10 mantissa bits\n",
    "- Typical FP32 values: `0.123456789`\n",
    "- FP16 representation: `0.1234` (lower precision but good enough for inference)\n",
    "\n",
    "---\n",
    "\n",
    "#### 📉 Why Use FP16?\n",
    "\n",
    "| Feature        | FP32                     | FP16                    |\n",
    "|----------------|--------------------------|-------------------------|\n",
    "| Memory usage   | 4 bytes per value         | 2 bytes per value       |\n",
    "| Compute speed  | Slower on GPUs            | Much faster on GPUs (especially A100/H100) |\n",
    "| Energy usage   | Higher                    | Lower                   |\n",
    "| Precision      | High                      | Slightly reduced (acceptable for inference) |\n",
    "\n",
    "🧠 FP16 helps run **large models** on GPUs with limited memory (e.g., 24GB vs 80GB cards).\n",
    "\n",
    "---\n",
    "\n",
    "#### ⚙️ What Is O1 Optimization?\n",
    "\n",
    "`O1` is a setting from **[DeepSpeed](https://www.deepspeed.ai/)** and **[Accelerate](https://huggingface.co/docs/accelerate)** used for **mixed-precision inference/training**.\n",
    "\n",
    "| Optimization Level | Description                           |\n",
    "|--------------------|---------------------------------------|\n",
    "| O0                 | Full precision (FP32)                 |\n",
    "| **O1**             | **Mixed precision (auto FP16 + FP32 fallback)** |\n",
    "| O2                 | Pure FP16                             |\n",
    "| O3                 | Advanced optimizations (e.g., quantization, kernel fusion) |\n",
    "\n",
    "##### 🔧 What Does O1 Do?\n",
    "- Automatically **casts compatible operations** (like matmul) to FP16\n",
    "- **Keeps numerically sensitive ops** (e.g., layer norm, softmax) in FP32\n",
    "- Result: **Best balance** between speed and stability\n",
    "\n",
    "---\n",
    "\n",
    "#### ⚡ Benefits at Inference Time\n",
    "\n",
    "| Metric            | Before (FP32 / O0) | After (FP16 / O1) |\n",
    "|------------------|---------------------|--------------------|\n",
    "| VRAM usage       | High                | ~2x lower          |\n",
    "| Batch size limit | Smaller             | Larger             |\n",
    "| Latency          | Higher              | Lower              |\n",
    "| Throughput       | Lower               | Higher             |\n",
    "\n",
    "**Example:** Running LLaMA-7B in FP32 might require ~30GB VRAM, while FP16 can bring that down to ~16GB.\n",
    "\n",
    "---\n",
    "\n",
    "#### 💡 Code Example (Hugging Face Transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"openchat/openchat-3.5-1210\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,  # 👈 Enable half-precision\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🧪 Bonus: Combine with O3\n",
    "O3 goes further with quantization, sparse attention, and custom kernels\n",
    "\n",
    "Supported by tools like DeepSpeed, vLLM, AWQ, and TensorRT\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
